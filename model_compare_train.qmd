---
title: "Theatre Case Duration Prediction (New)"
subtitle: "Subtitle"
author: "COCH Data & Analytics Team"
date: today
format:
  html:
    code-fold: true
    toc: true
    toc-float: true
    embed-resources: true
execute:
  warning: false
  message: false
---
```{python}
#| echo: false
#| message: false
#| warning: false

# NOTE: to output a dated version of the file to the correct folder (recommended) run 
# quarto render model_compare_train.qmd --output model_compare_train_250916.html
# Move-Item model_compare_train_250916.html "S:\Finance & Performance\IM&T\BIReporting\Data science projects\Case Duration - Theatre case duration prediction\model_compare_train\model_compare_train_yymmdd.html"
# Unfortunately there is no way of specifying this in the document, alternatively just manually rename and move the output file.
```

```{python}
#| echo: false
#| message: false
#| warning: false

# import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from xgboost import XGBRegressor
from category_encoders import TargetEncoder
from sklearn.metrics import mean_absolute_percentage_error, make_scorer, mean_absolute_error
from sklearn.base import BaseEstimator, RegressorMixin, clone
import matplotlib.pyplot as plt
import seaborn as sns
import pyodbc
import joblib
from datetime import date
from english_words import get_english_words_set
import random
```

## Training Notes
Capped numerical variables at 95th percentile.


```{python}
#| echo: false
# data import
dsn = "coch_p2" 

read_connection = pyodbc.connect(f'DSN={dsn}', autocommit=True)

sql_query = "select * from InformationSandpitDB.datascience.CaseDuration_training"
data = pd.read_sql_query(sql_query, read_connection)

read_connection.close()

# data.to_csv('dataset2.csv', index=False)

# data import
#data = pd.read_csv('dataset2.csv')
```

## Exploratory Data Analysis

### Target feature compared to baseline estimate

The target feature (the one we want to estimate) is CaseDuration - the duration of theatre cases. @fig-target (left) shows the actual distribution of case durations. There are existing estimates available on the waitlist (although their source remains mysterious to me). These will be used as a baseline to compare the model estimates with. @fig-target (right) shows the distributions of these estimates.

```{python}
#| label: fig-target
#| fig-cap: "Histogram of case durations"

# Ensure zero is included as a bin edge
min_val = min(data['CaseDuration'].min(), data['CernerEstimate'].min(), 0)
max_val = max(data['CaseDuration'].max(), data['CernerEstimate'].max())
bins = np.linspace(min_val, max_val, 61)  # 40 bins

fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharex=True)
sns.histplot(data['CaseDuration'], kde=True, bins=bins, ax=axes[0], color='skyblue')
axes[0].set_title('Distribution of CaseDuration')
axes[0].set_xlabel('CaseDuration')

sns.histplot(data['CernerEstimate'], kde=True, bins=bins, ax=axes[1], color='salmon')
axes[1].set_title('Distribution of CernerEstimate')
axes[1].set_xlabel('CernerEstimate')

axes[0].set_xlim(min_val, max_val)
axes[1].set_xlim(min_val, max_val)
plt.tight_layout()
plt.show()

# Get bin counts and edges for CaseDuration
# counts, bin_edges = np.histogram(data['CaseDuration'], bins=bins)

# # Get bin counts and edges for CernerEstimate
# counts_cerner, bin_edges_cerner = np.histogram(data['CernerEstimate'], bins=bins)

# bin_df = pd.DataFrame({
#     'Bin_LeftEdge': bin_edges[:-1],
#     'Bin_RightEdge': bin_edges[1:],
#     'CaseDuration_Count': counts,
#     'CernerEstimate_Count': counts_cerner
# })

# bin_df.head(100)
```


### Feature summary table

@tbl-feature-summary shows a summary of all the numerical features. The min and max values appear to lie within plausible ranges (which were determined in the SQL).

```{python}
#| label: tbl-feature-summary
#| tbl-cap: "Summary of features"
# specify a list of features to drop
drop_features = ['ip_encntr_id', 'op_encntr_id', 'ReferralID', 'SnapshotDate', 'TFSpecialty', 'HasGatekeeper', 'HasPreop']
data = data.drop(drop_features, axis = 1)
feature_summary = data.describe()
feature_summary = feature_summary.round(2)

feature_summary
```

### Missing data

@fig-eda-missing graphically shows which features contain more missing data and where missingness correlates across features. As expected all the fields which come from the gatekeeper or pre op assessments are highly correlated for missingness, as if no assessment has happened none of the fields are populated.

```{python}
#| label: fig-eda-missing
#| fig-cap: "Heatmap to show missing data."
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.show()
```

### Assigning features to categories

```{python}
#| code-fold: false
# split features into categories for plotting and pipeline
categorical_low_cardinality = ['PatientClassification', 'GatekeeperOutcome', 'PatientSex', 'Consultant']
categorical_high_cardinality = ['WaitListOpName', 'OPCSCategory'] # split high card into 2 categories - to treat differently in target encoding
numerical_features = ['PatientAge', 'PatientBMI', 'SystolicBloodPressure', 'DiastolicBloodPressure', 'HeartRateMonitored', 'DaysAgo']
binary_features = ['Injection']
```

### Binary features

Shows the average value of binary features in the dataset.
```{python}
#| label: fig-binary
#| fig-cap: "Average values of binary figures"
# Binary features bar chart
binary_avg = data[binary_features].mean().sort_values()
plt.figure(figsize=(10, 6))
sns.barplot(x=binary_avg.values, y=binary_avg.index, palette='viridis')
plt.xlabel('Proportion of Rows = 1')
plt.ylabel('Feature Name')
plt.title('Average Values of Binary Features')
plt.show()
```

### Numerical featues

@fig-numerical shows the distribution of the numerical features, which all look plausible and normal (with the exception of age). This means they are suitable for imputation using the mean in the pipeline.

```{python}
#| label: fig-numerical
#| fig-cap: "Average values of numerical features"

for column in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[column], kde=True, bins=30)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.show()
```

### Categorical features

@fig-categorical shows there are some procedures that are not well represented in the training data. This could cause potentially cause issues later on. The first 3 characters of the OPCS code have now been included to try and encompass a less granular categorisation for procedure and this does look like it has reduced the number of unique procedures (see @tbl-waitlistop and @tbl-opcs).

```{python}
#| label: fig-categorical
#| fig-cap: "Average values of numerical features"

all_cat_features = categorical_low_cardinality + categorical_high_cardinality
all_cat_features.remove('WaitListOpName')
all_cat_features.remove('OPCSCategory')

for column in all_cat_features:
    # Calculate category counts, sorted by frequency
    order = data[column].value_counts().index
    plt.figure(figsize=(8, 4))
    sns.countplot(data[column], order = order)
    plt.title(f'Barplot of {column}')
    plt.xlabel(column)
    plt.show()

order = data['WaitListOpName'].value_counts().index
plt.figure(figsize=(8, 40))
sns.countplot(data['WaitListOpName'], order = order)

#order = data['OPCSCategory'].value_counts().index
#plt.figure(figsize=(8, 30))
#sns.countplot(data['OPCSCategory'], order = order)
```

```{python}
#| label: tbl-waitlistop
#| tbl-cap: "Count of distinct procedure names"
counts_df = data['WaitListOpName'].value_counts().reset_index()
counts_df.columns = ['WaitListOpName', 'Count']
counts_df
```

```{python}
#| label: tbl-opcs
#| tbl-cap: "Count of distinct procedure names"
#counts_df = data['OPCSCategory'].value_counts().reset_index()
#counts_df.columns = ['OPCSCategory', 'Count']
#counts_df
```

### Cross Correlation between numerical features

@fig-correlation shows in general numerical features are not highly correlated, which is good as it suggests there is little redundancy. The highest correlation is between systolic and diastolic blood pressure, which is expected as they are measuring the same phenomenon.

```{python}
#| label: fig-correlation
#| fig-cap: "Cross Correlation Heatmap"
features_for_corr = numerical_features + binary_features + ['CaseDuration']
plt.figure(figsize=(12, 10))
corr = data[features_for_corr].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Cross Correlation Heatmap')
plt.show()
```


## Processing Pipeline

### Create processing pipeline

These steps specify the handling of missing variables, encoding of categorical variables and scaling. Note that with categorical variables missing data is encoded as None, whereas numerical variables it is encoded as np.nan. The imputer will only pick up np.nan by default and needs to be told to look for None if that is what you want.

```{python}
#| code-fold: false
def cap_at_95th_percentile(X):
    return np.apply_along_axis(lambda x: np.clip(x, None, np.percentile(x, 95)), axis=0, arr=X)

numeric_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('cap', FunctionTransformer(cap_at_95th_percentile, feature_names_out='one-to-one')),
        ('scaler', StandardScaler())
    ]
)

categorical_low_transformer = Pipeline(
    steps=[
        #('imputer', SimpleImputer(strategy='constant', fill_value='Missing', missing_values=None)),
        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),  
        #missing data in this column is not np.nan, but python None
        ('onehot', OneHotEncoder(
            handle_unknown='ignore',
            min_frequency=0.01)
        )
    ]
)

categorical_high_transformer = Pipeline(
    steps=[
        #('imputer', SimpleImputer(strategy='constant', fill_value='Missing', missing_values=None)),
        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),
        ('target_enc', TargetEncoder(min_samples_leaf=0)),
        ('scaler', StandardScaler())
    ]
)


preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat_low', categorical_low_transformer, categorical_low_cardinality),
        ('cat_high', categorical_high_transformer, categorical_high_cardinality),
        ('binary', 'passthrough', binary_features)
    ]
)
```

### Test Processing Pipeline

This is what the data looks like after its been passed through the pipeline. The tables below demonstrate all the columns are being passed through and processed in the manner specified. All the numerical and target encoded variables are scaled.

```{python}
#| label: tbl-inspect
#| tbl-cap: "Post pipeline table of training data for inspection"
# define target and features
y = data['CaseDuration']
X = data[categorical_low_cardinality + categorical_high_cardinality + binary_features + numerical_features]

# split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and pre-process the training data
X_train_preprocessed = preprocessor.fit_transform(X_train, y_train)

# pre process the test data
X_test_preprocessed = preprocessor.transform(X_test)

# convert them back to dataframes with meaningful column names so I can inspect
feature_names = preprocessor.get_feature_names_out()
X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=feature_names)

rename_mapping = {
    f'cat_high__{i}': col_name
    for i, col_name in enumerate(categorical_high_cardinality)
}

X_train_preprocessed_df = X_train_preprocessed_df.rename(columns=rename_mapping)

X_train_preprocessed_df
```

```{python}
#| label: tbl-inspect-test
#| tbl-cap: "Post pipeline table of test data for inspection"
X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, columns=feature_names)

X_test_preprocessed_df = X_test_preprocessed_df.rename(columns=rename_mapping)

X_test_preprocessed_df
```

## Model Comparison

The next section compares several different models over a range of hyperparameters (nested cross validation), to find the model that has the best and most stable performance when trained/tested on different slices of the dataset. When the best modelling procedure has been decided on the hyperparameter search will be repeated using the whole of the dataset.

*   **Ridge Regression:** A linear regression model that uses a tuning parameter to shrink the co-efficient estimates towards 0. This technique benefits from standardising the scale of the predictors first. Increasing the tuning parameter decreases the coefficients, reducing the flexibility of the fit, decreasing variance but increasing bias. (Avoiding overfitting, but making it less able to represent complex relationships between predictors).
*   **Random Forest Regressor:** An ensemble method that fits multiple decision trees on different sub-samples of the dataset and uses averaging to improve predictive accuracy and control overfitting.
*   **XGBoost Regressor:** A powerful and efficient implementation of gradient boosting.
*   **Gradient Boosting Regressor:** Another ensemble technique that builds models in a sequential manner, with each new model correcting the errors of the previous one.

```{python}
#| code-fold: false
#| label: set parameters
# Define models
models = {
    'Ridge Regression': Ridge(),
    'Random Forest': RandomForestRegressor(),
    'XGBoost': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Define hyperparameter grids for random search
param_distributions = {
    'Ridge Regression': {
        'model__alpha': [0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0] #0 is with no regularisation
    },
    'Random Forest': {
        'model__n_estimators': [50, 100, 200],
        'model__max_depth': [None, 5, 10, 20],
        'model__min_samples_split': [2, 5, 10],
        'model__min_samples_leaf': [1, 2, 4],
        'model__max_features': ['auto', 'sqrt']
    },
    'XGBoost': {
        'model__n_estimators': [50, 100, 200, 400, 800, 1100],
        'model__max_depth': [3, 4, 5, 6, 7],
        'model__learning_rate': [0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1],
        'model__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
        'model__colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        'model__min_child_weight': [1, 2, 4, 6],
        'model__gamma': [0, 0.5, 1, 2],
        'model__reg_lambda': [0.5, 1, 2, 5, 10],
        'model__reg_alpha': [0, 0.1, 0.5, 1, 2]
    },
    'Gradient Boosting': {
        'model__n_estimators': [50, 100, 200],
        'model__max_depth': [3, 5, 7],
        'model__learning_rate': [0.01, 0.05, 0.1],
        'model__subsample': [0.6, 0.8, 1.0],
        'model__max_features': ['auto', 'sqrt']
    }
}#

```

```{python}
#| label: make scorers

def percentage_within_10(y_true, y_pred):
    epsilon = 1e-10
    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon)) <= 0.1)

def percent_overage(y_true, y_pred):
    epsilon = 1e-10
    return np.mean((y_pred - y_true) / (y_true + epsilon) > 0.1)

def percent_underage(y_true, y_pred):
    epsilon = 1e-10
    return np.mean((y_pred - y_true) / (y_true + epsilon) < -0.1)

mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)

mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)

# Define the custom scoring dictionary
scoring = {
    'MAE': mae_scorer,
    'MAPE': mape_scorer,
    'Percentage Within 10%': make_scorer(percentage_within_10, greater_is_better=True),
    'Percent Overage': make_scorer(percent_overage, greater_is_better=False),
    'Percent Underage': make_scorer(percent_underage, greater_is_better=False)
}
```

```{python}
#| label: score baseline estimates

baseline_data = data[data['CernerEstimate'].notna()]

# Compute evaluation metrics
mae = mean_absolute_error(baseline_data['CaseDuration'], baseline_data['CernerEstimate'])
mape = mean_absolute_percentage_error(baseline_data['CaseDuration'], baseline_data['CernerEstimate'])
pct_within_10 = percentage_within_10(baseline_data['CaseDuration'], baseline_data['CernerEstimate'])
pct_overage = percent_overage(baseline_data['CaseDuration'], baseline_data['CernerEstimate'])
pct_underage = percent_underage(baseline_data['CaseDuration'], baseline_data['CernerEstimate'])

# Create a dictionary for baseline metrics
baseline_metrics = {
    'MAE': mae,
    'MAPE': mape,
    'Percentage Within 10%': pct_within_10,
    'Percent Overage': pct_overage,
    'Percent Underage': pct_underage
}

# Convert to DataFrame
baseline_df = pd.DataFrame({'Cerner Estimate': baseline_metrics}).T
```

```{python}
#| label: split data and prep for nested validation

# Split data into features and target
y = data['CaseDuration']
X = data[categorical_low_cardinality + categorical_high_cardinality + numerical_features + binary_features]

# Define outer cross-validation strategy
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize dictionary to store results
outer_results = {}

# Initialize dictionary to store absolute errors for visualization
actual_errors = {}

# Initialize dictionary to store percentage errors for visualization
percentage_errors = {}

# Percentage errors for baseline
fold_percentage_errors = []
epsilon = 1e-10
percentage_error = (baseline_data['CernerEstimate'] - baseline_data['CaseDuration'].values) / (baseline_data['CaseDuration'].values + epsilon)
fold_percentage_errors.extend(percentage_error)
percentage_errors['CernerEstimate'] = np.array(fold_percentage_errors)

# Absolute errors for baseline
fold_actual_errors = []
actual_error = baseline_data['CernerEstimate'] - baseline_data['CaseDuration'].values
fold_actual_errors.extend(actual_error)
actual_errors['CernerEstimate'] = np.array(fold_actual_errors)
```

```{python}
#| label: nested validation
for model_name, model in models.items():
    print(f"Evaluating {model_name} with nested cross-validation...")
    # Lists to store metrics for each fold
    mae_scores = []
    mape_scores = []
    pct_within_10_scores = []
    pct_overage_scores = []
    pct_underage_scores = []
    fold_percentage_errors = []
    fold_actual_errors = []

    # Outer loop: model evaluation
    for train_index, test_index in outer_cv.split(X, y):
        X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]
        y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]

        # Create pipeline
        pipeline = Pipeline(
            steps=[
                ('preprocessor', preprocessor),
                ('model', model)
            ]
        )

        # Hyperparameter tuning (inner cross-validation)
        if model_name in param_distributions:
            inner_cv = KFold(n_splits=3, shuffle=True, random_state=23)
            random_search = RandomizedSearchCV(
                estimator=pipeline,
                param_distributions=param_distributions[model_name],
                n_iter=30,
                cv=inner_cv,
                scoring=mape_scorer,
                random_state=11,
                n_jobs=1,
                refit=True #this is the default - fits on the outer fold
            )
            random_search.fit(X_train_outer, y_train_outer)
            best_model = random_search.best_estimator_
            print(f"Best hyperparameters for {model_name}: {random_search.best_params_}")
        else:
            # No hyperparameter tuning
            best_model = pipeline.fit(X_train_outer, y_train_outer)

        # Predict on the outer test set
        y_pred_outer = best_model.predict(X_test_outer)

        # Calculate percentage errors for visualization
        epsilon = 1e-10
        percentage_error = (y_pred_outer - y_test_outer.values) / (y_test_outer.values + epsilon)
        actual_error = y_pred_outer - y_test_outer.values
        fold_percentage_errors.extend(percentage_error)
        fold_actual_errors.extend(actual_error)

        # Compute evaluation metrics
        mae = mean_absolute_error(y_test_outer, y_pred_outer)
        mape = mean_absolute_percentage_error(y_test_outer, y_pred_outer)
        pct_within_10 = percentage_within_10(y_test_outer, y_pred_outer)
        pct_overage = percent_overage(y_test_outer, y_pred_outer)
        pct_underage = percent_underage(y_test_outer, y_pred_outer)

        # Append metrics
        mae_scores.append(mae)
        mape_scores.append(mape)
        pct_within_10_scores.append(pct_within_10)
        pct_overage_scores.append(pct_overage)
        pct_underage_scores.append(pct_underage)

    # Store the metrics
    outer_results[model_name] = {
        'MAE': {'mean': np.mean(mae_scores), 'std': np.std(mae_scores)},
        'MAPE': {'mean': np.mean(mape_scores), 'std': np.std(mape_scores)},
        'Percentage Within 10%': {'mean': np.mean(pct_within_10_scores), 'std': np.std(pct_within_10_scores)},
        'Percent Overage': {'mean': np.mean(pct_overage_scores), 'std': np.std(pct_overage_scores)},
        'Percent Underage': {'mean': np.mean(pct_underage_scores), 'std': np.std(pct_underage_scores)},
    }

    # Store percentage errors for visualization
    percentage_errors[model_name] = np.array(fold_percentage_errors)
    actual_errors[model_name] = np.array(fold_actual_errors)

# Display results
print("\nCross-Validation Results:")
for model_name, metrics in outer_results.items():
    print(f"\nModel: {model_name}")
    for metric_name, metric_values in metrics.items():
        print(f"{metric_name}: {metric_values['mean']:.4f} Â± {metric_values['std']:.4f}")

print("\nBaseline Metrics:")
for metric_name, metric_value in baseline_metrics.items():
    print(f"{metric_name}: {metric_value:.4f}")

# Convert results to DataFrame for plotting
results_df = pd.DataFrame({
    model_name: {metric_name: metrics[metric_name]['mean'] for metric_name in metrics}
    for model_name, metrics in outer_results.items()
}).T

ci_df = pd.DataFrame({
    model_name: {metric_name: metrics[metric_name]['std'] for metric_name in metrics}
    for model_name, metrics in outer_results.items()
}).T
```
```{python}
#| label: fig-model-scores
#| fig-cap: "Comparison of scoring metrics between the models"

# Convert results to DataFrame for plotting
results_df = pd.DataFrame({
    model_name: {metric_name: metrics[metric_name]['mean'] for metric_name in metrics}
    for model_name, metrics in outer_results.items()
}).T

ci_df = pd.DataFrame({
    model_name: {metric_name: metrics[metric_name]['std'] for metric_name in metrics}
    for model_name, metrics in outer_results.items()
}).T

# Concatenate baseline_df above results_df
results_df = pd.concat([baseline_df, results_df], axis=0)
ci_df.loc['Cerner Estimate'] = [0] * len(ci_df.columns) 
ci_df = ci_df.reindex(results_df.index)

results_df.plot(kind='bar', subplots=True, layout=(2, 3), figsize=(14, 10), legend=False, yerr=ci_df, capsize=5)
plt.tight_layout()
plt.show()

mini_results_df = results_df.loc[['Cerner Estimate', 'XGBoost'], ['MAE', 'MAPE']]
mini_ci_df = ci_df.loc[['Cerner Estimate', 'XGBoost']]

mini_results_df.plot(kind='bar', subplots=True, layout=(2, 1), figsize=(4, 8), legend=False, yerr=ci_df, capsize=5)
plt.tight_layout()
plt.show()
```


```{python}
#| label: fig-error-distribution
#| fig-cap: "Comparison of error distributions between the models"

fig, axes = plt.subplots(2, 3, figsize=(14, 10))
axes = axes.flatten()

for ax, (model_name, errors) in zip(axes, percentage_errors.items()):
    # Convert to percentage
    percentage_error_percent = errors * 100
    # Clip percentage errors to range -100% to +100%
    percentage_error_percent = np.clip(percentage_error_percent, -100, 100)
    # Plot histogram
    ax.hist(
        percentage_error_percent,
        bins=50,
        range=(-100, 100),
        weights=np.ones_like(percentage_error_percent) / len(percentage_error_percent),
        edgecolor='black'
    )
    ax.set_xlim(-100, 100)
    ax.set_xlabel('Percentage Error (%)')
    ax.set_ylabel('Proportion of Predictions')
    ax.set_title(f'Distribution of Errors: {model_name}')

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-error-distribution-unclipped
#| fig-cap: "Comparison of error distributions between the models (unclipped)"

fig, axes = plt.subplots(2, 3, figsize=(14, 10))
axes = axes.flatten()

for ax, (model_name, errors) in zip(axes, percentage_errors.items()):
    # Convert to percentage
    percentage_error_percent = errors * 100
    # Clip percentage errors to range -100% to +100%
    #percentage_error_percent = np.clip(percentage_error_percent, -100, 100)
    # Plot histogram
    ax.hist(
        percentage_error_percent,
        bins=50,
        #range=(-100, 100),
        weights=np.ones_like(percentage_error_percent) / len(percentage_error_percent),
        edgecolor='black'
    )
    #ax.set_xlim(-100, 100)
    ax.set_xlabel('Percentage Error (%)')
    ax.set_ylabel('Proportion of Predictions')
    ax.set_title(f'Distribution of Errors: {model_name}')

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-actual-error-distribution-unclipped
#| fig-cap: "Comparison of error distributions between the models (unclipped)"

fig, axes = plt.subplots(2, 3, figsize=(14, 10))
axes = axes.flatten()

for ax, (model_name, errors) in zip(axes, actual_errors.items()):
    # Convert to percentage
    #percentage_error_percent = errors * 100
    # Clip percentage errors to range -100% to +100%
    #percentage_error_percent = np.clip(percentage_error_percent, -100, 100)
    # Plot histogram
    ax.hist(
        errors,
        bins=50,
        #range=(-100, 100),
        weights=np.ones_like(errors) / len(errors),
        edgecolor='black'
    )
    #ax.set_xlim(-100, 100)
    ax.set_xlabel('Actual Error')
    ax.set_ylabel('Proportion of Predictions')
    ax.set_title(f'Distribution of Actual Errors: {model_name}')

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-actual-error-distribution-clipped
#| fig-cap: "Comparison of error distributions between the models (unclipped)"

fig, axes = plt.subplots(2, 3, figsize=(14, 10))
axes = axes.flatten()

for ax, (model_name, errors) in zip(axes, actual_errors.items()):
    # Convert to percentage
    #percentage_error_percent = errors * 100
    # Clip percentage errors to range -100% to +100%
    errors = np.clip(errors, -100, 100)
    # Plot histogram
    ax.hist(
        errors,
        bins=50,
        #range=(-100, 100),
        weights=np.ones_like(errors) / len(errors),
        edgecolor='black'
    )
    #ax.set_xlim(-100, 100)
    ax.set_xlabel('Actual Error (Clipped)')
    ax.set_ylabel('Proportion of Predictions')
    ax.set_title(f'Dist. of Actual Errors (Clipped): {model_name}')

plt.tight_layout()
plt.show()
```


The best / most consistent modelling process identified in the previous section is then selected (model name needs to be manually selected) and the selected model/hyperparameters are used on the full dataset to produce the final model.

```{python}
# Set to true if wanted to produce new version of model, and depending on results of comparison you may want to change training process
retrain = True

if retrain == True:
    # Get the model and its parameter grid
    final_model_name = 'XGBoost'
    final_model = models[final_model_name]
    final_param_dist = param_distributions[final_model_name]

    # Create the final pipeline
    final_pipeline = Pipeline(
        steps=[
            ('preprocessor', preprocessor),
            ('model', final_model)
        ]
    )

    # Run the hyperparameter search ONCE on the ENTIRE dataset
    # This is the same as the "inner loop" of the nested cross validation
    final_search = RandomizedSearchCV(
        estimator=final_pipeline,
        param_distributions=final_param_dist,
        n_iter=40, # This should maybe be increased
        cv=KFold(n_splits=3, shuffle=True, random_state=23),
        scoring=mape_scorer,
        random_state=11,
        n_jobs=1
    )

    # Fit on ALL the data
    final_search.fit(X, y)
    final_best_model = final_search.best_estimator_
    final_best_params = final_search.best_params_

    print(f"\n--- Final Model Training Complete ---")
    print(f"Final Model Type: {final_model_name}")
    print(f"Final Model Hyperparameters: {final_best_params}")

    # name the model using a random word
    web2lowerset = get_english_words_set(['web2'], lower=True)
    # Weed out the more ridiculous words
    min_length = 4
    max_length = 8
    common_length_words = [
        word for word in web2lowerset
        if min_length <= len(word) <= max_length
    ]
    random_word = random.choice(list(common_length_words))

    model_file_name = f'{date.today()}_{random_word}.joblib'
    joblib.dump(final_best_model, model_file_name)
    print(f"Model saved as: {model_file_name}")
```

### SHAP
```{python}
#| label: shap
#| fig-cap: "SHAP"
if retrain == True:
    import shap
    # Extract the model and preprocessor from the pipeline
    final_model_only = final_best_model.named_steps['model']
    preprocessor_only = final_best_model.named_steps['preprocessor']

    # Preprocess the full dataset
    X_preprocessed = preprocessor_only.transform(X)

    # Get feature names from preprocessor
    feature_names = preprocessor_only.get_feature_names_out()

    # Convert to DataFrame with feature names
    X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=feature_names)

    # Rename columns
    rename_mapping = {
        f'cat_high__{i}': col_name
        for i, col_name in enumerate(categorical_high_cardinality)
    }
    for col in X_preprocessed_df.columns:
        if col not in rename_mapping:  # Skip if already mapped
            # Remove prefix and underscores
            if '__' in col:
                prefix, feature_name = col.split('__', 1)
                rename_mapping[col] = feature_name
    X_preprocessed_df = X_preprocessed_df.rename(columns=rename_mapping)

    # Create SHAP explainer
    explainer = shap.TreeExplainer(final_model_only)
    shap_values = explainer.shap_values(X_preprocessed_df)

    # Create summary plot with feature names
    #shap.summary_plot(shap_values, X_preprocessed_df, plot_type="bar")
    #plt.tight_layout()
    #plt.show()

    # Optional: Create a more detailed summary plot
    shap.summary_plot(shap_values, X_preprocessed_df)
    plt.tight_layout()
    plt.show()
```